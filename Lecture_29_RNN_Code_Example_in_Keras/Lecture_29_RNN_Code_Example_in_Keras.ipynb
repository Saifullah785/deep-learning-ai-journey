{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP1heXnY1Ab+RlpXCMw8Gt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saifullah785/deep-learning-ai-journey/blob/main/Lecture_29_RNN_Code_Example_in_Keras/Lecture_29_RNN_Code_Example_in_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN Sentiment Analysis**"
      ],
      "metadata": {
        "id": "tZInIG3UYiId"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7ubz282g44E5"
      },
      "outputs": [],
      "source": [
        "# In this cell, we are creating a list of documents (or sentences) that we will use for our sentiment analysis task.\n",
        "# Each string in the list represents a separate document.\n",
        "import numpy as np\n",
        "\n",
        "docs = ['go pakistan',\n",
        "        'pakistan go',\n",
        "        'hip hip hurray',\n",
        "        'jeetega bhai jeetega pakistan jeetega',\n",
        "        'pakistan zindabad',\n",
        "        'afridi afridi',\n",
        "        'qahid ki qahid'\n",
        "        'hum koi ghulam hai',\n",
        "        'pti jeetega',\n",
        "        'imran khan zindabad',\n",
        "        'oh sab de mah a gai he',\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we are importing the Tokenizer class from the tensorflow.keras.preprocessing.text module.\n",
        "# The Tokenizer is used to vectorize a text corpus, by turning each text into either a sequence of integers or into a vector.\n",
        "# We are initializing a Tokenizer object and specifying an out-of-vocabulary (oov) token.\n",
        "# The oov_token is used to represent words that are not in the vocabulary of the tokenizer.\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<nothings>')"
      ],
      "metadata": {
        "id": "usQgdaaF6PN2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell, we are training the tokenizer on our list of documents.\n",
        "# The fit_on_texts method updates the internal vocabulary based on the list of texts provided.\n",
        "tokenizer.fit_on_texts(docs)"
      ],
      "metadata": {
        "id": "EnfvcE7z6VbE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays the word_index attribute of the tokenizer.\n",
        "# The word_index is a dictionary that maps words to their integer representations.\n",
        "# The keys are the words and the values are the corresponding integers.\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY0osT8iXhCu",
        "outputId": "25753b73-27fc-4af0-e97d-ee9d98c025fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<nothings>': 1,\n",
              " 'pakistan': 2,\n",
              " 'jeetega': 3,\n",
              " 'go': 4,\n",
              " 'hip': 5,\n",
              " 'zindabad': 6,\n",
              " 'afridi': 7,\n",
              " 'hurray': 8,\n",
              " 'bhai': 9,\n",
              " 'qahid': 10,\n",
              " 'ki': 11,\n",
              " 'qahidhum': 12,\n",
              " 'koi': 13,\n",
              " 'ghulam': 14,\n",
              " 'hai': 15,\n",
              " 'pti': 16,\n",
              " 'imran': 17,\n",
              " 'khan': 18,\n",
              " 'oh': 19,\n",
              " 'sab': 20,\n",
              " 'de': 21,\n",
              " 'mah': 22,\n",
              " 'a': 23,\n",
              " 'gai': 24,\n",
              " 'he': 25}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays the word_counts attribute of the tokenizer.\n",
        "# The word_counts is an ordered dictionary that contains the counts of each word in the documents.\n",
        "# The keys are the words and the values are their frequencies.\n",
        "tokenizer.word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2PeBQAyXmHW",
        "outputId": "ada1bea2-e562-4540-d3cc-7bb2c2a6ca76"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('go', 2),\n",
              "             ('pakistan', 4),\n",
              "             ('hip', 2),\n",
              "             ('hurray', 1),\n",
              "             ('jeetega', 4),\n",
              "             ('bhai', 1),\n",
              "             ('zindabad', 2),\n",
              "             ('afridi', 2),\n",
              "             ('qahid', 1),\n",
              "             ('ki', 1),\n",
              "             ('qahidhum', 1),\n",
              "             ('koi', 1),\n",
              "             ('ghulam', 1),\n",
              "             ('hai', 1),\n",
              "             ('pti', 1),\n",
              "             ('imran', 1),\n",
              "             ('khan', 1),\n",
              "             ('oh', 1),\n",
              "             ('sab', 1),\n",
              "             ('de', 1),\n",
              "             ('mah', 1),\n",
              "             ('a', 1),\n",
              "             ('gai', 1),\n",
              "             ('he', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays the document_count attribute of the tokenizer.\n",
        "# The document_count is the number of documents that the tokenizer was trained on.\n",
        "tokenizer.document_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT2tcP7sXqWl",
        "outputId": "c838ba53-9d88-4544-8f72-a3796393a879"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell, we are converting our list of documents into a list of sequences of integers.\n",
        "# The texts_to_sequences method of the tokenizer is used for this purpose.\n",
        "# Each document is converted into a sequence of integers based on the word_index.\n",
        "sequences = tokenizer.texts_to_sequences(docs)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaIexoBHXwDb",
        "outputId": "025fa62b-4dc4-497c-b2f2-675ad2c97190"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 2],\n",
              " [2, 4],\n",
              " [5, 5, 8],\n",
              " [3, 9, 3, 2, 3],\n",
              " [2, 6],\n",
              " [7, 7],\n",
              " [10, 11, 12, 13, 14, 15],\n",
              " [16, 3],\n",
              " [17, 18, 6],\n",
              " [19, 20, 21, 22, 23, 24, 25]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell calculates the size of the vocabulary.\n",
        "# The vocabulary size is the number of unique words in the documents, plus the oov_token if specified.\n",
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-bN6ybQ6ZSa",
        "outputId": "0630f5fc-0586-4cc8-b3fd-f8f00c18158e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we are importing the pad_sequences function from the keras.utils module.\n",
        "# The pad_sequences function is used to ensure that all sequences in a list have the same length.\n",
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "B5dX3TNG6w1d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell, we are padding our sequences to ensure they all have the same length.\n",
        "# We are using 'post' padding, which means that zeros will be added to the end of the sequences.\n",
        "sequences = pad_sequences(sequences,padding='post')\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D68iwYzW6y-t",
        "outputId": "961d7c3e-1fc5-4c12-cec6-43e06eb82a8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  0,  0,  0,  0,  0],\n",
              "       [ 5,  5,  8,  0,  0,  0,  0],\n",
              "       [ 3,  9,  3,  2,  3,  0,  0],\n",
              "       [ 2,  6,  0,  0,  0,  0,  0],\n",
              "       [ 7,  7,  0,  0,  0,  0,  0],\n",
              "       [10, 11, 12, 13, 14, 15,  0],\n",
              "       [16,  3,  0,  0,  0,  0,  0],\n",
              "       [17, 18,  6,  0,  0,  0,  0],\n",
              "       [19, 20, 21, 22, 23, 24, 25]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are importing the necessary modules for building our RNN model.\n",
        "# We import the imdb dataset, the Sequential model, and the Dense, SimpleRNN, and Embedding layers.\n",
        "from keras.datasets import imdb\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding,Flatten"
      ],
      "metadata": {
        "id": "TCKHyTgs7omP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell, we are loading the IMDB dataset.\n",
        "# The IMDB dataset is a large collection of movie reviews, pre-processed and ready to be used for sentiment analysis.\n",
        "# The data is already split into training and testing sets.\n",
        "(X_train,y_train),(X_test,y_test) = imdb.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdDp2J9SX9QW",
        "outputId": "4a1ea2db-f931-4525-c827-c884bdde4eaa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays the first training example from the IMDB dataset.\n",
        "# The training examples are sequences of integers, where each integer represents a word.\n",
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAjrpT_9YADy",
        "outputId": "a14e3bc7-fb70-47d1-a216-9db1124f5bcd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell calculates the length of the third training example in the IMDB dataset.\n",
        "len(X_train[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjUn0NK6YFu0",
        "outputId": "0f7a13ea-22dd-42e4-e90a-728a1d6e538b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "141"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell, we are padding both the training and testing sequences to a maximum length of 50.\n",
        "# We are using 'post' padding, which means that zeros will be added to the end of the sequences.\n",
        "# We are also truncating sequences that are longer than 50.\n",
        "X_train = pad_sequences(X_train,padding='post',maxlen=50)\n",
        "X_test = pad_sequences(X_test,padding='post',maxlen=50)"
      ],
      "metadata": {
        "id": "A6oqjo9iYFry"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays the first training example after padding.\n",
        "# You can see that the sequence now has a length of 50.\n",
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yZha3z7YFoL",
        "outputId": "1f9fe5f2-1559-41ab-e1a5-56bd8808ec10"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2071,   56,   26,  141,    6,  194, 7486,   18,    4,  226,   22,\n",
              "         21,  134,  476,   26,  480,    5,  144,   30, 5535,   18,   51,\n",
              "         36,   28,  224,   92,   25,  104,    4,  226,   65,   16,   38,\n",
              "       1334,   88,   12,   16,  283,    5,   16, 4472,  113,  103,   32,\n",
              "         15,   16, 5345,   19,  178,   32], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell, we are defining our SimpleRNN model.\n",
        "# We are using a Sequential model, which is a linear stack of layers.\n",
        "# The first layer is a SimpleRNN layer with 32 units. The input_shape is (50, 1) because our sequences have a length of 50 and we are feeding one feature at a time.\n",
        "# The second layer is a Dense layer with a single output unit and a 'sigmoid' activation function, which is suitable for binary classification.\n",
        "model = Sequential()\n",
        "\n",
        "model.add(SimpleRNN(32,input_shape=(50,1),return_sequences=False))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "YF5wfq_1YFlO",
        "outputId": "4de96974-fb44-445f-b7e4-f91101c159b9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,121\u001b[0m (4.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,121</span> (4.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,121\u001b[0m (4.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,121</span> (4.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell, we are compiling our model.\n",
        "# We are using 'binary_crossentropy' as the loss function, which is suitable for binary classification problems.\n",
        "# We are using the 'adam' optimizer, which is a popular and effective optimization algorithm.\n",
        "# We are also specifying that we want to track the 'accuracy' metric during training.\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UxXqXfAjYFVR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell, we are training our model.\n",
        "# We are using the fit method of the model to train it on the training data (X_train, y_train).\n",
        "# We are training for 5 epochs, which means that the model will see the entire training dataset 5 times.\n",
        "# We are also providing validation data (X_test, y_test) to evaluate the model's performance on unseen data after each epoch.\n",
        "model.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywPeApYWYSPr",
        "outputId": "b4b906c0-535f-4a13-eb13-f50d0c807f81"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - accuracy: 0.4983 - loss: 0.7297 - val_accuracy: 0.5040 - val_loss: 0.6934\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 13ms/step - accuracy: 0.5067 - loss: 0.6935 - val_accuracy: 0.5070 - val_loss: 0.6944\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.5149 - loss: 0.6919 - val_accuracy: 0.5028 - val_loss: 0.6937\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.5079 - loss: 0.6926 - val_accuracy: 0.5008 - val_loss: 0.6940\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - accuracy: 0.5091 - loss: 0.6923 - val_accuracy: 0.5028 - val_loss: 0.6939\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e49fa641450>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0556CzY1YSGz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}